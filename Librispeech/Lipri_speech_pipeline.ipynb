{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMG9//+0e7D3DoCbPSoLd6V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohannadEhabBarakat/text2audio/blob/main/code/datasets/Lipri_speech_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of4X3MZ_OHOm"
      },
      "source": [
        "# Librispeech Data pipline \r\n",
        "\r\n",
        "LibriSpeech is a corpus of approximately 1000 hours of 16kHz read English speech, prepared by Vassil Panayotov with the assistance of Daniel Povey. The data is derived from read audiobooks from the LibriVox project, and has been carefully segmented and aligned. \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmVXSeKxRp5D"
      },
      "source": [
        "###Needed pakages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5k8KzuxOqPY"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8-5Bm-FSqFw"
      },
      "source": [
        "def load_dataset(path):\r\n",
        "  \"\"\"\r\n",
        "  load the dataset using tfds\r\n",
        "\r\n",
        "  Arguments:\r\n",
        "  path -- path to save the dataset on \r\n",
        "\r\n",
        "  Return:\r\n",
        "  train_clean, train_noise, dev_clean, dev_noise, test_clean, tast_noise -- data splits \r\n",
        "  \"\"\"\r\n",
        "  train_clean, train_noise, dev_clean, dev_noise, test_clean, tast_noise= tfds.load('Librispeech',\r\n",
        "                                                                                    split=['train_clean360', 'train_other500', 'dev_clean', 'dev_other', 'test_clean', 'test_other' ], \r\n",
        "                                                                                    as_supervised=True, data_dir=file_path) \r\n",
        "  \r\n",
        "  return train_clean, train_noise, dev_clean, dev_noise, test_clean, tast_noise\r\n",
        "  \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RGEA3mqaVF7"
      },
      "source": [
        "def concatinate_data(split1, split2):\r\n",
        "  \"\"\"\r\n",
        "  concatinate two dataset's splis \r\n",
        "\r\n",
        "  Arguments:\r\n",
        "  split1, split2 -- two splits to be concatinated  \r\n",
        "\r\n",
        "  Return:\r\n",
        "  concatinated -- A Tensor resulting from concatenation of the input splits  \r\n",
        "  \"\"\"\r\n",
        "  concatinated = tf.concat([split1, split2], axis = 0)\r\n",
        "  return concatinated\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZf9OYHg9zaT"
      },
      "source": [
        "def dataset_format (dataset_split, text_to_speech = False):\r\n",
        "  \"\"\"\r\n",
        "  reverse the dataset format from (speech, text) tuple to (text, speech) if text to speech is true \r\n",
        "  \r\n",
        "  Arguments:\r\n",
        "  dataset_split -- dataset \r\n",
        "  text_to_speech  -- boolien value if true will reverse input and labels  \r\n",
        "\r\n",
        "  Return:\r\n",
        "  same dataset if speech to text, reversed tuple if text to speech \r\n",
        "  \"\"\"\r\n",
        "  if text_to_speech:\r\n",
        "    return (dataset_split[1], dataset_split[0])\r\n",
        "  else:\r\n",
        "    return dataset_split\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7uZPbyab6e_"
      },
      "source": [
        "def pipeline (clean_data, noisy_data , clean_only = True, shuffle = True ,\r\n",
        "              batch_size = 32, buffer_size= 1000, text_to_speech = False):\r\n",
        "  \"\"\"\r\n",
        "  apply shuffiling and batching to the input data  \r\n",
        "\r\n",
        "  Arguments:\r\n",
        "  clean_data -- clean split of the data\r\n",
        "  noisy_data -- noisy split of the data \r\n",
        "  clean_only -- boolien if true pipeline will be applied to clean split only\r\n",
        "  shuffle -- boolien if true data will be shuffled \r\n",
        "  batch_size -- integer for the  batch size\r\n",
        "  buffer_size -- integer for the buffer size for shuffeling \r\n",
        "  text_to_speech  -- boolien value if true will reverse the dataset format from (speech, text) tuple to (text, speech)\r\n",
        "  \r\n",
        "  Return:\r\n",
        "  data -- suffiled and batched data \r\n",
        "  \"\"\"\r\n",
        "  if clean_only:\r\n",
        "    data = clean_data\r\n",
        "    if shuffle: \r\n",
        "      data   = data.shuffle(buffer_size)\r\n",
        "    data = data.map(lambda speech, text: dataset_format(data, text_to_speech = text_to_speech )\r\n",
        "    data = data.batch(batch_size)\r\n",
        "    data = data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n",
        "\r\n",
        "  else:\r\n",
        "    data = concatinate_data(clean_data, noisy_data)\r\n",
        "    \r\n",
        "    if shuffle: \r\n",
        "      data   = data.shuffle(buffer_size)\r\n",
        "\r\n",
        "    data = data.map(lambda speech, text: dataset_format(data, text_to_speech = text_to_speech )\r\n",
        "    data = data.batch(batch_size)\r\n",
        "    data = data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n",
        "\r\n",
        "      \r\n",
        "  return data\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUmRhlgnb7MP"
      },
      "source": [
        "def dataset_loader_and_formatter (path, clean_only = True, shuffle = True,\r\n",
        "              batch_size_train = 32, batch_size_val = 32, batch_size_test = 32, buffer_size= 1000, text_to_speech = False):\r\n",
        "  \r\n",
        "  \"\"\"\r\n",
        "  load all dataset splits and apply pipeline to them   \r\n",
        "\r\n",
        "  Arguments:\r\n",
        "  path -- path to save the dataset \r\n",
        "  clean_only -- boolien if true pipeline will be applied to clean splits only\r\n",
        "  shuffle -- boolien if true data will be shuffled \r\n",
        "  batch_size -- integer for the  batch size for each of the train, val, and test sets\r\n",
        "  buffer_size -- integer for the buffer size for shuffeling \r\n",
        "  text_to_speech  -- boolien value if true will reverse the dataset format from (speech, text) tuple to (text, speech)\r\n",
        "\r\n",
        "  Return:\r\n",
        "  train_data -- ready to use dataset\r\n",
        "  val_data  -- ready to use dataset\r\n",
        "  test_data  -- ready to use dataset\r\n",
        "  \"\"\"\r\n",
        "  \r\n",
        "  train_clean, train_noise, dev_clean, dev_noise, test_clean, tast_noise = load_dataset(path)\r\n",
        "\r\n",
        "  train_data = pipeline (train_clean, train_noise , clean_only = clean_only, shuffle = shuffle ,\r\n",
        "                        batch_size = batch_size_train, buffer_size= buffer_size, text_to_speech = text_to_speech )\r\n",
        "  \r\n",
        "  val_data = pipeline (dev_clean, dev_noise , clean_only = clean_only, shuffle = shuffle ,\r\n",
        "                       batch_size = batch_size_val, buffer_size= buffer_size, text_to_speech = text_to_speech )\r\n",
        "  \r\n",
        "  test_data = pipeline (test_clean, tast_noise , clean_only = clean_only, shuffle = shuffle ,\r\n",
        "                       batch_size = batch_size_test, buffer_size= buffer_size, text_to_speech = text_to_speech )\r\n",
        "  \r\n",
        "  return train_data, val_data, test_data\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}