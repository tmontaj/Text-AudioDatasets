{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " data download and save",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tmontaj/Text-AudioDatasets/blob/main/Librispeech/data_download_and_save.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35WEEAreDkGF"
      },
      "source": [
        "## Downloading and preparing Librispeech dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw1czforDyDe"
      },
      "source": [
        "##### needed libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1knC1n0aprd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f401237a-97e5-4551-e52e-f62594f8bdc5"
      },
      "source": [
        "import pandas as pd\n",
        "import tarfile\n",
        "import os, sys\n",
        "import shutil\n",
        "\n",
        "\n",
        "!pip install wget\n",
        "import wget\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=d23f1decf8ce620d876c938939336ca143e387c6f6670714092646263c35f5fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBYuj4Iy6lM3"
      },
      "source": [
        "#create this bar_progress method which is invoked automatically from wget and used in deffrent code\n",
        "\n",
        "def _bar_progress(current, total, width=80):\n",
        "  progress_message = \"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total)\n",
        "  # Don't use print() as it will print in new line every time.\n",
        "  sys.stdout.write(\"\\r\" + progress_message)\n",
        "  sys.stdout.flush()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euFVSxbIETG8"
      },
      "source": [
        "##### Downloading and extracting Librispeech "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN-42XODzpXR"
      },
      "source": [
        "def download_librispeech(out, splits):\n",
        "  \"\"\"\n",
        "    Downloading librispeech dataset splits\n",
        "\n",
        "    Arguments:\n",
        "    out -- path to save the dataset on\n",
        "    splits -- list of splits needed to be downloaded. splits are:\n",
        "                    [dev-clean\n",
        "                    dev-other,\n",
        "                    test-clean, \n",
        "                    test-other,\n",
        "                    train-clean-100,\n",
        "                    train-clean-360,\n",
        "                    train-other-500]\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  def _splits_url(split_name):\n",
        "    return \"https://www.openslr.org/resources/12/\"+split_name+\".tar.gz\"\n",
        "  \n",
        "  def _splits_progress(split_name, split_number, splits_count):\n",
        "    progress_message = \"Split: %s [%d / %d]\" % (split_name, split_number, splits_count)\n",
        "    # Don't use print() as it will print in new line every time.\n",
        "    sys.stdout.write(\"\\r\" + progress_message+\"\\n\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "  split_number = 1\n",
        "  splits_count = len(splits)\n",
        "\n",
        "  for split_name in splits:\n",
        "    _splits_progress(split_name, split_number, splits_count)\n",
        "    wget.download(_splits_url(split_name), out=out, bar=_bar_progress)\n",
        "    split_number+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyqlTt8dsCKt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6adb99b7-e07a-42a9-eb39-681d38783900"
      },
      "source": [
        "# download_librispeech(\"\", [\"dev-clean\", \"dev-other\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: dev-clean [1 / 2]\n",
            "Downloading: dev-other [2 / 2]\n",
            "Downloading: 100% [314305928 / 314305928] bytes"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-VL_gxc6ohV"
      },
      "source": [
        "def unzip_librispeech(out, extract_path):\n",
        "  \"\"\"\n",
        "  extracting librispeech data\n",
        "\n",
        "  Arguments:\n",
        "  out -- path of the downloaded tar files \n",
        "  extract_path -- path to extract the files on  \n",
        "  \"\"\"\n",
        "  dirs = os.listdir(out)\n",
        "\n",
        "  print(\"Start extracting ...\")\n",
        "\n",
        "  for i in dirs:\n",
        "    target_name = i.split('.')\n",
        "    name = out +'/'+i\n",
        "    if name.endswith('.tar.gz'):\n",
        "      tar = tarfile.open(i, \"r:gz\")\n",
        "      tar.extractall(extract_path +'/' + target_name[0])\n",
        "      tar.close()\n",
        "\n",
        "  print(\"... Finished extracting\")\n",
        "    \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2tmqlW02LlT"
      },
      "source": [
        "# unzip_librispeech(\".\", \"tst\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtvyPJtxLH_q"
      },
      "source": [
        "##### Organize directories "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzBOOxZGLN1E"
      },
      "source": [
        "def organize_dirs (extract_path, organized_path):\n",
        "  \"\"\"\n",
        "  extracting librispeech data\n",
        "\n",
        "  Arguments:\n",
        "  extract_path -- path to extract the files on  \n",
        "  organized_path -- path to organize the files in  \n",
        "  \"\"\"\n",
        "  print(\"Start organize_dirs ...\")\n",
        "\n",
        "  dirs = os.listdir(extract_path)\n",
        "  for dir in dirs:\n",
        "    shutil.move(extract_path+ '/'+ dir+ '/' + 'LibriSpeech/'+ dir , organized_path)\n",
        "  \n",
        "  common_files_path = extract_path + '/' + dirs[0]+'/' + \"LibriSpeech\"\n",
        "  dirs = os.listdir( common_files_path )\n",
        "\n",
        "  for f in dirs:\n",
        "    shutil.move(common_files_path+'/'+ f , organized_path)\n",
        "  \n",
        "  print(\"... Finished organize_dirs\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCfpoqWaLwfK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "200ffc6d-4086-4374-e9f3-8813506a5c0f"
      },
      "source": [
        "# organize_dirs (\"./tst\", \"./tst2\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev-clean\n",
            "dev-other\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS0wI8yX5MoN"
      },
      "source": [
        "def _remove(dir_path):\n",
        "  \"\"\"\n",
        "  thin wrapper over os.system to remove directory or file \n",
        "\n",
        "  Arguments:\n",
        "  dir_path -- path to dirctory or file to remove  \n",
        "  \"\"\"\n",
        "  os.system('rm -R %s' %dir_path)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s827HZ3x3-q"
      },
      "source": [
        "def _rename(dir_path, old_name, new_name):\n",
        "  \"\"\"\n",
        "  thin wrapper over os.system to rename directory or file \n",
        "\n",
        "  Arguments:\n",
        "  dir_path -- path to dirctory or file to rename  \n",
        "  old_name -- old name (original) for directory or file\n",
        "  new_name -- new name for directory or file\n",
        "  \"\"\"\n",
        "  os.system('mv %s %s' %(dir_path+\"/\"+old_name, dir_path+\"/\"+new_name))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28cImkxyzkFE"
      },
      "source": [
        "# _rename(\"./\", \"SPEAKERS2.TXT\", \"SPEAKERS3.TXT\")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UQdPZNY61Ib"
      },
      "source": [
        "def download_and_extract(out, splits, extract_path, organized_path, remove_organized_path=False):\n",
        "  \"\"\"\n",
        "  download and extract librispeech\n",
        "\n",
        "  Arguments:\n",
        "  out -- path of the downloaded tar files \n",
        "  extract_path -- path to extract the files on  \n",
        "  organized_path -- path to organize the files in  \n",
        "  remove_organized_path -- flag to remove organized_path (uses -R to remove all files)\n",
        "  splits -- list of splits needed to be downloaded. splits are:\n",
        "                    [dev-clean\n",
        "                    dev-other,\n",
        "                    test-clean, \n",
        "                    test-other,\n",
        "                    train-clean-100,\n",
        "                    train-clean-360,\n",
        "                    train-other-500]\n",
        "  \"\"\"\n",
        "  download_librispeech(out, splits)\n",
        "  unzip_librispeech(out, extract_path)\n",
        "  if clean_organized_path:\n",
        "    _remove(organized_path)\n",
        "  organize_dirs (extract_path, organized_path)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuoadGhF31o2"
      },
      "source": [
        "download_and_extract(out = \"./\",\n",
        "                     splits = [\"dev-clean\", \"dev-other\"]\n",
        "                     extract_path = \"./tst\",\n",
        "                     organized_path = \"./tst2\"\n",
        "                     )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaRFaQn54hNb"
      },
      "source": [
        "def load(src,splits):\n",
        "  \"\"\"\n",
        "  simple download and extract librispeech\n",
        "\n",
        "  Arguments:\n",
        "  src -- path to dataset directory \n",
        "  splits -- list of splits needed to be downloaded. splits are:\n",
        "                    [dev-clean\n",
        "                    dev-other,\n",
        "                    test-clean, \n",
        "                    test-other,\n",
        "                    train-clean-100,\n",
        "                    train-clean-360,\n",
        "                    train-other-500]\n",
        "  \"\"\"\n",
        "  os.system(\"mkdir %s\" %(src+\"/librispeech\"))\n",
        "\n",
        "  out = src+\"/out\"\n",
        "  extract_path = src+\"/tmp\"\n",
        "  organized_path = src+\"/dataset\"\n",
        "\n",
        "  download_and_extract(out=out,\n",
        "                     splits=splits,\n",
        "                     extract_path = extract_path, \n",
        "                     organized_path = organized_path\n",
        "                     )\n",
        "  print(\"CONGRATS Librispeach is ready to be used at %s\" %(organized_path))\n"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R6YgMEppxsu"
      },
      "source": [
        "def clean_speakers_file(src):\n",
        "  \"\"\"\n",
        "  clean speakers file\n",
        "\n",
        "  Arguments:\n",
        "  src -- path to dataset\n",
        "  \"\"\"\n",
        "  input=open(src+\"/SPEAKERS.TXT\", \"r\")\n",
        "  dest=open(src+\"/SPEAKERS_temp.TXT\", \"w\")\n",
        "\n",
        "  input_lines = input.readlines()\n",
        "\n",
        "  line_num = 1\n",
        "  for line in input_lines:\n",
        "    if line_num == 45:\n",
        "      line = line.split(\"|\")\n",
        "      line [-2] = line[-2]+\" \"+line[-1] \n",
        "      line.pop(-1)\n",
        "      line.pop(-2)\n",
        "      line = \"|\".join(line)\n",
        "\n",
        "    if line_num == 12:\n",
        "      line = line[1:].lower()\n",
        "    \n",
        "    dest.write(line)\n",
        "    line_num+=1\n",
        "\n",
        "  input.close()\n",
        "  dest.close()\n",
        "\n",
        "  # _remove(src+\"/SPEAKERS.TXT\")\n",
        "  _rename(src, \"SPEAKERS_temp.TXT\", \"speakers.txt\")"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9dBFWbJrz2u"
      },
      "source": [
        "clean_speakers_file(src=\"./\")"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzWtHWnSa_XB"
      },
      "source": [
        "def load_metadata(data_path):\n",
        "  \"\"\"\n",
        "  load metadata currently loads speakers.txt only \n",
        "  \n",
        "  Arguments:\n",
        "  data_path -- path to dataset\n",
        "  \"\"\"\n",
        "\n",
        "  # use sep | and skip first 11 rows \n",
        "  speakers = pd.read_csv(data_path+\"/\"+'speakers.txt', sep=\"|\", skiprows=11)\n",
        "  speakers.columns = speakers.columns.map(lambda x: x.strip())\n",
        "  speakers.set_index(\"id\", inplace=True)\n",
        "  return speakers"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uosOfDuE9Ai7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "44460f7d-7424-440c-fda1-23350e1f3d02"
      },
      "source": [
        "# x = load_metadata(\"./\")\n",
        "# x.columns\n",
        "\n",
        "load_metadata(\"./\")"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sex</th>\n",
              "      <th>subset</th>\n",
              "      <th>minutes</th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>F</td>\n",
              "      <td>train-clean-360</td>\n",
              "      <td>25.03</td>\n",
              "      <td>Kristin LeMoine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>F</td>\n",
              "      <td>train-clean-360</td>\n",
              "      <td>25.11</td>\n",
              "      <td>Alys AtteWater</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>M</td>\n",
              "      <td>train-clean-360</td>\n",
              "      <td>25.04</td>\n",
              "      <td>Gord Mackenzie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>F</td>\n",
              "      <td>train-clean-100</td>\n",
              "      <td>25.19</td>\n",
              "      <td>Kara Shallenberg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>F</td>\n",
              "      <td>train-other-500</td>\n",
              "      <td>30.07</td>\n",
              "      <td>Gesine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8975</th>\n",
              "      <td>F</td>\n",
              "      <td>train-clean-100</td>\n",
              "      <td>25.11</td>\n",
              "      <td>Daisy Flaim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9000</th>\n",
              "      <td>M</td>\n",
              "      <td>train-other-500</td>\n",
              "      <td>27.26</td>\n",
              "      <td>Ramon Escamilla</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9022</th>\n",
              "      <td>F</td>\n",
              "      <td>train-clean-360</td>\n",
              "      <td>25.17</td>\n",
              "      <td>Claire M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9023</th>\n",
              "      <td>F</td>\n",
              "      <td>train-clean-360</td>\n",
              "      <td>25.19</td>\n",
              "      <td>P. J. Morgan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9026</th>\n",
              "      <td>F</td>\n",
              "      <td>train-clean-360</td>\n",
              "      <td>21.75</td>\n",
              "      <td>Tammy Porter</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2484 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      sex              subset  minutes               name\n",
              "id                                                       \n",
              "14     F    train-clean-360      25.03    Kristin LeMoine\n",
              "16     F    train-clean-360      25.11     Alys AtteWater\n",
              "17     M    train-clean-360      25.04     Gord Mackenzie\n",
              "19     F    train-clean-100      25.19   Kara Shallenberg\n",
              "20     F    train-other-500      30.07             Gesine\n",
              "...   ...                 ...      ...                ...\n",
              "8975   F    train-clean-100      25.11        Daisy Flaim\n",
              "9000   M    train-other-500      27.26    Ramon Escamilla\n",
              "9022   F    train-clean-360      25.17           Claire M\n",
              "9023   F    train-clean-360      25.19       P. J. Morgan\n",
              "9026   F    train-clean-360      21.75       Tammy Porter\n",
              "\n",
              "[2484 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHBxE8sU9Gm_"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(filename,names=['code'])\n",
        "df[['code','name_of_code']] = df[\"code\"].str.split(\" \", 1, expand=True)\n",
        "df[\"name_of_code\"] = df[\"name_of_code\"].str.strip(\"-\")\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ukEE09B0WdR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}