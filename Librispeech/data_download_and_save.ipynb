{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " data download and save",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tmontaj/Text-AudioDatasets/blob/main/Librispeech/data_download_and_save.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35WEEAreDkGF"
      },
      "source": [
        "## Downloading and preparing Librispeech dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw1czforDyDe"
      },
      "source": [
        "##### needed libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1knC1n0aprd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e132492d-0ec7-4cfb-a834-5eb86c4b0cc7"
      },
      "source": [
        "import pandas as pd\n",
        "import tarfile\n",
        "import os, sys\n",
        "import shutil\n",
        "\n",
        "\n",
        "!pip install wget\n",
        "import wget\n",
        "!pip install soundfile\n",
        "import soundfile\n",
        "from pathlib import Path"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBYuj4Iy6lM3"
      },
      "source": [
        "#create this bar_progress method which is invoked automatically from wget and used in deffrent code\n",
        "\n",
        "def _bar_progress(current, total, width=80):\n",
        "  progress_message = \"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total)\n",
        "  # Don't use print() as it will print in new line every time.\n",
        "  sys.stdout.write(\"\\r\" + progress_message)\n",
        "  sys.stdout.flush()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euFVSxbIETG8"
      },
      "source": [
        "##### Downloading and extracting Librispeech "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN-42XODzpXR"
      },
      "source": [
        "def download_librispeech(out, splits):\n",
        "  \"\"\"\n",
        "    Downloading librispeech dataset splits\n",
        "\n",
        "    Arguments:\n",
        "    out -- path to save the dataset on\n",
        "    splits -- list of splits needed to be downloaded. splits are:\n",
        "                    [dev-clean\n",
        "                    dev-other,\n",
        "                    test-clean, \n",
        "                    test-other,\n",
        "                    train-clean-100,\n",
        "                    train-clean-360,\n",
        "                    train-other-500]\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  def _splits_url(split_name):\n",
        "    return \"https://www.openslr.org/resources/12/\"+split_name+\".tar.gz\"\n",
        "  \n",
        "  def _splits_progress(split_name, split_number, splits_count):\n",
        "    progress_message = \"Split: %s [%d / %d]\" % (split_name, split_number, splits_count)\n",
        "    # Don't use print() as it will print in new line every time.\n",
        "    sys.stdout.write(\"\\r\" + progress_message+\"\\n\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "  print(\"Start downloading librispeech ...\")\n",
        "  split_number = 1\n",
        "  splits_count = len(splits)\n",
        "\n",
        "  for split_name in splits:\n",
        "    _splits_progress(split_name, split_number, splits_count)\n",
        "    wget.download(_splits_url(split_name), out=out, bar=_bar_progress)\n",
        "    split_number+=1\n",
        "\n",
        "  print(\"... Finish downloading librispeech\")\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyqlTt8dsCKt"
      },
      "source": [
        "# download_librispeech(\"\", [\"dev-clean\", \"dev-other\"])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-VL_gxc6ohV"
      },
      "source": [
        "def unzip_librispeech(out, extract_path):\n",
        "  \"\"\"\n",
        "  extracting librispeech data\n",
        "\n",
        "  Arguments:\n",
        "  out -- path of the downloaded tar files \n",
        "  extract_path -- path to extract the files on  \n",
        "  \"\"\"\n",
        "  dirs = os.listdir(out)\n",
        "\n",
        "  print(\"Start extracting ...\")\n",
        "\n",
        "  for i in dirs:\n",
        "    target_name = i.split('.')\n",
        "    name = out +'/'+i\n",
        "    if name.endswith('.tar.gz'):\n",
        "      tar = tarfile.open(i, \"r:gz\")\n",
        "      tar.extractall(extract_path +'/' + target_name[0])\n",
        "      tar.close()\n",
        "\n",
        "  print(\"... Finished extracting\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2tmqlW02LlT"
      },
      "source": [
        "# unzip_librispeech(\".\", \"tst\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtvyPJtxLH_q"
      },
      "source": [
        "##### Organize directories "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzBOOxZGLN1E"
      },
      "source": [
        "def organize_dirs (extract_path, organized_path):\n",
        "  \"\"\"\n",
        "  extracting librispeech data\n",
        "\n",
        "  Arguments:\n",
        "  extract_path -- path to extract the files on  \n",
        "  organized_path -- path to organize the files in  \n",
        "  \"\"\"\n",
        "  print(\"Start organize_dirs ...\")\n",
        "\n",
        "  dirs = os.listdir(extract_path)\n",
        "  for dir in dirs:\n",
        "    shutil.move(extract_path+ '/'+ dir+ '/' + 'LibriSpeech/'+ dir , organized_path)\n",
        "  \n",
        "  common_files_path = extract_path + '/' + dirs[0]+'/' + \"LibriSpeech\"\n",
        "  dirs = os.listdir( common_files_path )\n",
        "\n",
        "  for f in dirs:\n",
        "    shutil.move(common_files_path+'/'+ f , organized_path)\n",
        "  \n",
        "  print(\"... Finished organize_dirs\")\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCfpoqWaLwfK"
      },
      "source": [
        "# organize_dirs (\"./tst\", \"./tst2\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS0wI8yX5MoN"
      },
      "source": [
        "def _remove(dir_path):\n",
        "  \"\"\"\n",
        "  thin wrapper over os.system to remove directory or file \n",
        "\n",
        "  Arguments:\n",
        "  dir_path -- path to dirctory or file to remove  \n",
        "  \"\"\"\n",
        "  os.system('rm -R %s' %dir_path)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s827HZ3x3-q"
      },
      "source": [
        "def _rename(dir_path, old_name, new_name):\n",
        "  \"\"\"\n",
        "  thin wrapper over os.system to rename directory or file \n",
        "\n",
        "  Arguments:\n",
        "  dir_path -- path to dirctory or file to rename  \n",
        "  old_name -- old name (original) for directory or file\n",
        "  new_name -- new name for directory or file\n",
        "  \"\"\"\n",
        "  os.system('mv %s %s' %(dir_path+\"/\"+old_name, dir_path+\"/\"+new_name))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28cImkxyzkFE"
      },
      "source": [
        "# _rename(\"./\", \"SPEAKERS2.TXT\", \"SPEAKERS3.TXT\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UQdPZNY61Ib"
      },
      "source": [
        "def download_and_extract(out, splits, extract_path,\n",
        "                         organized_path, remove_organized_path=False, download=True):\n",
        "  \"\"\"\n",
        "  download and extract librispeech\n",
        "\n",
        "  Arguments:\n",
        "  out -- path of the downloaded tar files \n",
        "  extract_path -- path to extract the files on  \n",
        "  organized_path -- path to organize the files in  \n",
        "  remove_organized_path -- flag to remove organized_path (uses -R to remove all files)\n",
        "  download -- flag to optionaly skip download the dataset\n",
        "  splits -- list of splits needed to be downloaded. splits are:\n",
        "                    [dev-clean\n",
        "                    dev-other,\n",
        "                    test-clean, \n",
        "                    test-other,\n",
        "                    train-clean-100,\n",
        "                    train-clean-360,\n",
        "                    train-other-500]\n",
        "  \"\"\"\n",
        "  if download:\n",
        "    download_librispeech(out, splits)\n",
        "  print(\"----------------------------\")\n",
        "  unzip_librispeech(out, extract_path)\n",
        "  print(\"----------------------------\")\n",
        "  if remove_organized_path:\n",
        "    _remove(organized_path)\n",
        "  organize_dirs (extract_path, organized_path)\n",
        "  print(\"----------------------------\")\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuoadGhF31o2"
      },
      "source": [
        "# download_and_extract(out = \"./\",\n",
        "#                      splits = [\"dev-clean\", \"dev-other\"],\n",
        "#                      extract_path = \"./tst\",\n",
        "#                      organized_path = \"./tst2\"\n",
        "#                      )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeeSAcOn-Dk3"
      },
      "source": [
        "# _remove(\"./tst\")\n",
        "# _remove(\"./tst2\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaRFaQn54hNb"
      },
      "source": [
        "def load(src, splits, remove_organized_path=False, download=True):\n",
        "  \"\"\"\n",
        "  simple download and extract librispeech\n",
        "\n",
        "  Arguments:\n",
        "  src -- path to dataset directory \n",
        "  splits -- list of splits needed to be downloaded. splits are:\n",
        "                    [dev-clean\n",
        "                    dev-other,\n",
        "                    test-clean, \n",
        "                    test-other,\n",
        "                    train-clean-100,\n",
        "                    train-clean-360,\n",
        "                    train-other-500]\n",
        "  \"\"\"\n",
        "  src = src+\"/librispeech\"\n",
        "  out = src+\"/out\"\n",
        "  extract_path = src+\"/tmp\"\n",
        "  organized_path = src+\"/data\"\n",
        "\n",
        "  os.system(\"mkdir -p %s\" %(src))\n",
        "  if download:\n",
        "    os.system(\"mkdir -p %s\" %(out))\n",
        "    _remove(out+\"/*\")\n",
        "\n",
        "  os.system(\"mkdir -p %s\" %(extract_path))\n",
        "  _remove(extract_path+\"/*\")\n",
        "\n",
        "\n",
        "  os.system(\"mkdir -p %s\" %(organized_path))\n",
        "  _remove(organized_path+\"/*\")\n",
        "\n",
        "\n",
        "  download_and_extract(out=out,\n",
        "                     splits=splits,\n",
        "                     extract_path = extract_path, \n",
        "                     organized_path = organized_path,\n",
        "                     remove_organized_path = remove_organized_path,\n",
        "                     download = download\n",
        "                     )\n",
        "  print(\"CONGRATS Librispeach is ready to be used at %s\" %(organized_path))\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3AtE4fK-UTI",
        "outputId": "27e0c93e-6bf2-405d-9a80-acf025d86c0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "load(src=\"dataset\",\n",
        "     splits=[\"dev-clean\", \"dev-other\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start downloading librispeech ...\n",
            "Split: dev-clean [1 / 2]\n",
            "Split: dev-other [2 / 2]\n",
            "Downloading: 75% [237928448 / 314305928] bytes"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R6YgMEppxsu"
      },
      "source": [
        "def clean_speakers_file(src):\n",
        "  \"\"\"\n",
        "  clean speakers file\n",
        "\n",
        "  Arguments:\n",
        "  src -- path to dataset\n",
        "  \"\"\"\n",
        "  input=open(src+\"/SPEAKERS.TXT\", \"r\")\n",
        "  dest=open(src+\"/SPEAKERS_temp.TXT\", \"w\")\n",
        "\n",
        "  input_lines = input.readlines()\n",
        "\n",
        "  line_num = 1\n",
        "  for line in input_lines:\n",
        "    if line_num == 45:\n",
        "      line = line.split(\"|\")\n",
        "      line [-2] = line[-2]+\" \"+line[-1] \n",
        "      line.pop(-1)\n",
        "      line.pop(-2)\n",
        "      line = \"|\".join(line)\n",
        "\n",
        "    if line_num == 12:\n",
        "      line = line[1:].lower()\n",
        "    \n",
        "    dest.write(line)\n",
        "    line_num+=1\n",
        "\n",
        "  input.close()\n",
        "  dest.close()\n",
        "\n",
        "  # _remove(src+\"/SPEAKERS.TXT\")\n",
        "  _rename(src, \"SPEAKERS_temp.TXT\", \"speakers.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9dBFWbJrz2u"
      },
      "source": [
        "# clean_speakers_file(src=\"./\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzWtHWnSa_XB"
      },
      "source": [
        "def load_metadata(data_path):\n",
        "  \"\"\"\n",
        "  load metadata currently loads speakers.txt only \n",
        "  \n",
        "  Arguments:\n",
        "  data_path -- path to dataset\n",
        "  \"\"\"\n",
        "\n",
        "  # use sep | and skip first 11 rows \n",
        "  speakers = pd.read_csv(data_path+\"/\"+'speakers.txt', sep=\"|\", skiprows=11)\n",
        "  speakers.columns = speakers.columns.map(lambda x: x.strip())\n",
        "  speakers.set_index(\"id\", inplace=True)\n",
        "  return speakers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uosOfDuE9Ai7"
      },
      "source": [
        "# x = load_metadata(\"./\")\n",
        "# x.columns\n",
        "\n",
        "# load_metadata(\"./\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHBxE8sU9Gm_"
      },
      "source": [
        "def load_trans(src):\n",
        "  \"\"\"\n",
        "  load single file of trans\n",
        "  \n",
        "  Arguments:\n",
        "  src -- path to the file\n",
        "  \"\"\"\n",
        "  split_name = \"dev-clean\"\n",
        "  split = split_name.split(\"-\")\n",
        "\n",
        "  df = pd.read_csv(src,names=['data'])\n",
        "  df[['id','text']] = df[\"code\"].str.split(\" \", 1, expand=True)\n",
        "  df[['speaker', 'chapter', 'index']] = df[\"id\"].str.strip(\"-\", expand=True)\n",
        "  df[[\"split\"]] = split[0]\n",
        "  df[[\"isClean\"]] = True if split[1] == \"clean\" else False\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "770MoncIJ__f"
      },
      "source": [
        "# load_trans(src)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl9QRSTMJzV-"
      },
      "source": [
        "def load_all_trans(src):\n",
        "  \"\"\"\n",
        "  load single file of trans\n",
        "  \n",
        "  Arguments:\n",
        "  src -- path to data directory\n",
        "  \"\"\"\n",
        "  splits = [x for x in Path(src).iterdir() if x.is_dir()]\n",
        "\n",
        "  for path in Path('src').rglob('*.trans.txt'):\n",
        "      print(path)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB843XEhOu84"
      },
      "source": [
        "#  load_all_trans(src)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAqHWng2RFnD"
      },
      "source": [
        "def load_wav(src, split, isClean, chapter, speaker, id):\n",
        "  split = split + (\"-clean\" if isClean else \"-other\")\n",
        "  path = os.path.join(src, split, chapter, speaker, id+\".flac\")\n",
        "  wav, sample_rate = sf.read(path)      \n",
        "\n",
        "  return wav, sample_rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwsKonbzTzu0"
      },
      "source": [
        "# wav, sample_rate = load_wav(src, split, isClean, chapter, speaker, id)\n",
        "# wav.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nXTpduwJPkQ"
      },
      "source": [
        "def load_sample(data, src, split, isClean, chapter, speaker, id):\n",
        "  data = data[data[\"split\"]==split]\n",
        "  data = data[data[\"isClean\"]==isClean]\n",
        "  data = data[data[\"id\"]==id]\n",
        "\n",
        "  wav, sample_rate = load_wav(src, split, isClean, chapter, speaker, id)\n",
        "\n",
        "  data[['wav', \"sample_rate\"]] = wav, sample_rate\n",
        "\n",
        "  return wav"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}